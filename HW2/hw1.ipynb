{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRbuIIjtMMFj"
      },
      "source": [
        "# CSCI 5525 Programming HW1\n",
        "\n",
        "**Authors**: Meredith Bain\n",
        "**consulted**: Anna Ton Nu\n",
        "\n",
        "**Emails**: bain0074@umn.edu\n",
        "**consulted**: tonnu016@umn.edu\n",
        "\n",
        "**Submission.** Please insert your names and emails above, save your code in this notebook, and explain what you are doing along with your findings in text cells. You can think of it as a technical report with code. Before submission, please use `Kernel -> Restart & Run All` in the Jupyter menu to verify your code is runnable and save all outputs. Afterwards, you can either upload your raw notebook (`hw1.ipynb`) or an exported PDF version to the `Homework 1` assignment in Canvas.\n",
        "\n",
        "\n",
        "In this homework, we will begin looking at diet data and formulate a diet prediction problem. Diet prediction is a historically important problem that helped shape optimization theory and machine learning. You can read about it here: [Dantzig Diet Optim Backstory](), and [Stigler Diet](https://en.m.wikipedia.org/wiki/Stigler_diet).\n",
        "\n",
        " The purpose of this homework is to:\n",
        "  - Data Manipulation let you practice different techniques that you can use to preprocess raw data.\n",
        "  - Prepare data for an ML objective\n",
        "  - Create a Neural Net Model to solve several problems.\n",
        "\n",
        "**Note**: You can use either local runtimes to complete this assignment, or a hosted runtime (with GPU) on Colab. The second option generally runs faster.  If using a runtime hosted on Colab, you can use the File Explorer pane on the left to upload the `epi_full.csv` file. Make sure to wait until the file finishes uploading before running the next code block.\n",
        "\n",
        "## Prepare Data\n",
        "The data is taken from kaggle: [Epicurious Dataset ML example](https://www.kaggle.com/datasets/hugodarwood/epirecipes), and there are several uses: [Epicurious Dataset ML example](https://www.kaggle.com/code/mathchi/2-lr-nb-knn-rf-predict-data-epicurious).  Feel free to examine and import ideas from kaggle uses. For convenience the dataset is included in the canvas.  Download from, upload to colab and load the data and understand the columns.  Each row is a meal, and the columns give: How good it tastes (Rating), Basic nutrition info (Macros = Calories, Carb, Fat, Protein), whether the meal is part of a named list (the X.type columns), Region information (e.g. Alabama or other place name), Ingredient names (Artichoke, beef, etc.), Meal Timing (Breakfast, Brunch, Lunch, Dinner), and several more.\n",
        "\n",
        "Your job is to:\n",
        "\n",
        "- group all columns that are food ingredients into ingredient vectors\n",
        "- create a list of all possible daily menus by forming a new list which takes all combinations of Breakfast, Lunch and Dinner (BLD) options.\n",
        "- For the $i^{th}$ daily menu BLD combination, add the three calory values, add the Macros, average the three ratings, and add the thee ingredient vectors.  The summed ingredient vector $x_i$ is our predictor.\n",
        "- The objective is to predict acceptable daily menus.  To be acceptable, we need to keep within the Daily Recommended Intake values for Cals and Macros, and serve meals in the top 25% of meal ratings. Look up your DRI values here: [DRI calculator](https://www.omnicalculator.com/health/dri). Note that the values have Upper and Lower values to the range.  Acceptable would be to stay in that range for all values.\n",
        "- Use the criteria above (ratings in upper 25% and meal plan within upper and lower DRI ranges) to create a target score vector $y$ by assigning to the $i^{th}$ meal plan $y_i=0$ if unacceptable and $y_i=1$ if acceptable.  \n",
        "- Learn to predict $y_i$ from $x_i$ using an MLP in Pytorch.  A good tutorial is here: [Dive into Deep Learning: MLP](https://d2l.ai/chapter_multilayer-perceptrons/mlp.html)\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "data = pd.read_csv('epi_full.csv')\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notes from class\n",
        "\n",
        "1. table join is needed to join in what nutrients these foods have, on average\n",
        "2. you can look up missing nutritional info externally if you want\n",
        "    a. looks like carbs are missing for all entries\n",
        "3. you may want to use a sparse tensor in pytorch to make stuff run faster\n",
        "4. 2 layers: test the output, then map\n",
        "5. Bayes optimal classification\n",
        "    a = log((p(x|C1)p(C1)) / (p(x|C2)p(C2))) log-odds\n",
        "    class posterior can be written as P(C1|x) = 1 / (1 + exp(-a)) = sigma(a)\n",
        "\n",
        "    softmax is the n-version of sigmoid function (k-class) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-09-22T21:19:22.818413Z",
          "start_time": "2021-09-22T21:19:18.899437Z"
        },
        "id": "YpsTjkMaMMFk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\"\"\"\n",
        "Below is data exploration scratch work to determine\n",
        "what needed to be done to clean the data. This is where \n",
        "column names were exported to determine which were \n",
        "ingredients in need of carb lookups & macro aggregations \n",
        "later.\n",
        "\"\"\"\n",
        "\n",
        "data = pd.read_csv('epi_full.csv')\n",
        "print(len(data))\n",
        "\n",
        "# print(data[\"X3.ingredient.recipes\"])\n",
        "\n",
        "# data[\"alaska\"].to_csv('alaska.csv')\n",
        "# data[\"almond\"].to_csv('almond.csv')\n",
        "# data[\"X3.ingredient.recipes\"].to_csv('X3.csv')\n",
        "\n",
        "# data.iloc[2:14].to_csv('sample_rows.csv')\n",
        "\n",
        "\n",
        "# with open('columns.csv', 'w') as f:\n",
        "#     for col in data.columns:\n",
        "#         print(col)\n",
        "#         f.write(col + '\\n')\n",
        "\n",
        "# print(data[data[\"drink\"] == 1][\"title\"])\n",
        "# print(data[data[\"drink\"] == 1][\"drinks\"])\n",
        "\n",
        "\n",
        "# print(data[data[\"drinks\"] == 1][\"title\"])\n",
        "# print(data[data[\"drinks\"] == 1][\"drink\"])\n",
        "# print(data[data[\"drinks\"] == 1][\"title\",\"drinks\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Explanation of work\n",
        "\n",
        "To begin with, the column names from the epi_full dataset werre exported to a separate CSV, where each row represented a header name from the original file. Ingredient columns were manually tagged with a 1, so that a filter could be made for the columns that need carb information looked up externally.\n",
        "\n",
        "The Kaggle dataset https://www.kaggle.com/datasets/utsavdey1410/food-nutrition-dataset?resource=download was used for carb information after being combined into a single CSV nutrition-facts-lookup.csv. Google Sheets was used to fuzzy match with a non-exact VLOOKUP, which was manually corrected where necessary. Where there was no match in the external table, the closest substitute available was used (for example, ground chicken for poultry sausage). \n",
        "\n",
        "Once the lookup information was complete, ingredients for each dish were collected in the epi_full dataset and macros and calories for each dish were calculated. \n",
        "\n",
        "Then, itertools was used to create all breakfast, lunch, and dinner combos, and calculate the daily macro/calories/rating and label for each one, stopping at ~2.1M combinations for the sake of time. This was the data that was fed into Pytorch (BLD.tsv).\n",
        "\n",
        "The prediction from the model does not seem to work well. Predictions may have been improved by using all of the 122M+ data combinations -- since only a small proportion of the data fit the criteria, it is possible that there were not enough 1's in the dataset. Different loss functions may have yielded more accurate results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mvs8Kh0dDJRZ"
      },
      "outputs": [],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#filter dataframe to ingredients\n",
        "cols = pd.read_csv('columns.csv')\n",
        "# print(cols)\n",
        "ingredient_filter = ['title']\n",
        "ingredient_filter += cols[cols['ingredient']==1][\"Column Name\"].to_list()\n",
        "\n",
        "print(ingredient_filter)\n",
        "all_ingredients = data.loc[:, data.columns.isin(ingredient_filter)]\n",
        "all_ingredients.to_csv('ingredients.csv')\n",
        "# print(ingredients)\n",
        "\n",
        "#make df of nutrition facts to look up carbs below\n",
        "nutrition_facts = pd.read_csv(\"combined-nutrition.csv\", index_col=False)\n",
        "nutrition_facts.index.name = None\n",
        "print(nutrition_facts)\n",
        "\n",
        "# print(data[data[\"title\"] == \"Beef Tenderloin with Garlic and Brandy \"])\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "this cell handles looking up the missing carb information\n",
        "and appending it to the original dataset\n",
        "\"\"\"\n",
        "\n",
        "#get the ingredients for a dish\n",
        "def get_ingredients(row):\n",
        "    ingredients = []\n",
        "    for ingredient in ingredient_filter:\n",
        "        # print(ingredient)\n",
        "        # print(row[ingredient].iloc[-1])\n",
        "        if row[ingredient].iloc[-1] == 1:\n",
        "            ingredients.append(ingredient)\n",
        "    return ingredients\n",
        "\n",
        "def create_ingredients_lists(data):\n",
        "    ingredients_lookup = {}\n",
        "    for title in data[\"title\"]:\n",
        "        print('title:',title)\n",
        "        row = data[data[\"title\"] == title]\n",
        "        print(row)\n",
        "        ingredients = get_ingredients(row)\n",
        "        ingredients_lookup[title] = ingredients\n",
        "    return ingredients_lookup\n",
        "\n",
        "#lookup carb values of foods\n",
        "def get_carbs(title_of_food, ingredients_lookup):\n",
        "    carb_crosswalk = pd.read_csv('nutrition-facts-lookup.csv')\n",
        "    carbs = 0\n",
        "    for ingredient in ingredients_lookup[title_of_food]:\n",
        "        # print(ingredient, carb_crosswalk[carb_crosswalk[\"title\"] == ingredient][\"manual match\"].iloc[-1])\n",
        "        lookup_value = carb_crosswalk[carb_crosswalk[\"title\"] == ingredient][\"manual match\"].iloc[-1]\n",
        "        carb = nutrition_facts[nutrition_facts[\"food\"] == lookup_value][\"Carbohydrates\"].iloc[-1]\n",
        "        # print(carb)\n",
        "        # print(type(carb))\n",
        "        carbs += float(carb)\n",
        "    # print(carbs)   \n",
        "    return carbs\n",
        "\n",
        "# create a new column in the original dataset with the carb values. \n",
        "# all_ingredients = all_ingredients[:1]\n",
        "all_ingredients.index.name = None\n",
        "print(all_ingredients)\n",
        "\n",
        "ingredients_lookup = create_ingredients_lists(all_ingredients)\n",
        "print(ingredients_lookup)\n",
        "carb_list = []\n",
        "for title in all_ingredients[\"title\"]:\n",
        "    print(title)\n",
        "    try:\n",
        "        carbs = get_carbs(title, ingredients_lookup)\n",
        "        carb_list.append(carbs)\n",
        "    except:\n",
        "        carb_list.append(0)\n",
        "\n",
        "# print(carb_list)\n",
        "\n",
        "# add in the carbs\n",
        "data[\"carbs\"] = carb_list\n",
        "data.to_csv('carbs_added.csv')\n",
        "\n",
        "data = pd.read_csv('carbs_added.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This cell sets the thresholds for determining whether a record\n",
        "fits the criteria (1) or not (0)\n",
        "\"\"\"\n",
        "\n",
        "#calculate the top 25% of ratings\n",
        "q1 = data[\"rating\"].quantile(0.25)\n",
        "print(q1)\n",
        "\n",
        "carbs_q2 = data[\"carbs\"].quantile(0.5)\n",
        "carbs_q3 = data[\"carbs\"].quantile(0.75)\n",
        "carbs_q4 = data[\"carbs\"].quantile(0.99)\n",
        "\n",
        "\n",
        "protein_q2 = data[\"protein\"].quantile(0.5)\n",
        "protein_q3 = data[\"protein\"].quantile(0.75)\n",
        "protein_q4 = data[\"protein\"].quantile(0.99)\n",
        "\n",
        "\n",
        "print(f\"middle carb range: {carbs_q2*3}-{carbs_q3*3}; {carbs_q4*3}\")\n",
        "print(f\"middle protein range: {protein_q2*3}-{protein_q3*3}; {protein_q4*3}\")\n",
        "\n",
        "\n",
        "#set macro boundaries\n",
        "calory_goal = 2285.2\n",
        "calory_min = 0.9 * calory_goal\n",
        "calory_max = 1.1 * calory_goal\n",
        "carb_min = 215\n",
        "carb_max = 371\n",
        "fat_min = 51\n",
        "fat_max = 89\n",
        "protein_min = 40\n",
        "protein_max = 141\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This cell creates the meal combinations and aggregate the nutrition information,\n",
        "and evaluates whether each record meets the criteria\n",
        "\"\"\"\n",
        "\n",
        "#make the BLD combos\n",
        "import itertools\n",
        "\n",
        "breakfasts = data[data[\"breakfast\"] == 1][\"title\"]\n",
        "lunches = data[data[\"lunch\"] == 1][\"title\"]\n",
        "dinners = data[data[\"dinner\"] == 1][\"title\"]\n",
        "\n",
        "print(\"Creating triplets\")\n",
        "print(len(breakfasts), len(lunches), len(dinners))\n",
        "triplets = list(itertools.product(breakfasts, lunches, dinners))\n",
        "print(f\"{len(triplets)} triplets created\")\n",
        "\n",
        "#look up calories, protien, fats, carbs, & ratings of a recipe\n",
        "def get_macros(triplet):\n",
        "    calories = carbs = fat = protein = rating = 0\n",
        "    top_25 = True\n",
        "    for title in triplet:\n",
        "        # print(title)\n",
        "        row = data[data[\"title\"] == title]\n",
        "        # print(\"row\", row)\n",
        "        calories += row[\"calories\"].iloc[0]\n",
        "        # print(\"calories:\", calories)\n",
        "        carbs += row[\"carbs\"].iloc[-1]\n",
        "        # print(\"carbs:\", carbs)\n",
        "        fat += row[\"fat\"].iloc[-1]\n",
        "        # print(\"fat:\", fat)\n",
        "        protein += row[\"protein\"].iloc[-1]\n",
        "        # print(\"protein:\", protein)\n",
        "        rating += row[\"rating\"].iloc[-1]\n",
        "\n",
        "        if row[\"rating\"].iloc[-1] < q1:\n",
        "            top_25 = False\n",
        "        # print(\"rating:\", rating)\n",
        "        # print()\n",
        "    \n",
        "    rating = rating / 3\n",
        "\n",
        "    if (not top_25) or (calories > calory_goal) or (protein < protein_min) or (carbs < carb_min) or (carbs > carb_max) or (fat < fat_min) or (fat > fat_max):\n",
        "        y = 0\n",
        "    else: \n",
        "        y = 1\n",
        "\n",
        "    return calories, carbs, fat, protein, rating, y\n",
        "\n",
        "i = 0\n",
        "\n",
        "\n",
        "#I stopped this around 2.1M rows, since running the whole 122M+ rows would have taken 43 hours.\n",
        "with open(\"BLD.tsv\", \"w\") as f:\n",
        "    f.write(\"combo\\tcalories\\tcarbs\\tfat\\tprotein\\trating\\ty\\n\")\n",
        "    for combo in triplets:\n",
        "        # print('combo', combo)\n",
        "        if i % 100000 == 0:\n",
        "            print(i)\n",
        "            if i == 210_000_000: #running for the full 122M+ combos will take too long\n",
        "                break\n",
        "        i += 1\n",
        "        calories, carbs, fat, protein, rating, y = get_macros(combo)\n",
        "        # if y == 1:\n",
        "        #     print(\"MEETS CRITERIA\")\n",
        "        f.write(f\"{combo}\\t{calories}\\t{carbs}\\t{fat}\\t{protein}\\t{rating}\\t{y}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\" \n",
        "this cell creates and trains the model using Pytorch\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import pandas as pd\n",
        "\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(10, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "BLD = pd.read_csv('BLD.tsv', sep='\\t')\n",
        "\n",
        "# turn data into X matrices and y labels\n",
        "X = BLD.iloc[:, 1:-1].values\n",
        "# print(X[1])\n",
        "y = BLD.iloc[:, -1].values   \n",
        "# print(y[1])\n",
        "\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "input_size = X.shape[1]\n",
        "print(\"input size:\", input_size)\n",
        "model = SimpleNet(input_size)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=512, shuffle=True)\n",
        "\n",
        "print(\"training model...\")\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    for inputs, labels in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = nn.BCEWithLogitsLoss()\n",
        "        # loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{epochs}]')#, Loss: {loss.item():.4f}')\n",
        "\n",
        "print('Training finished!')\n",
        "\n",
        "#predict\n",
        "test_input = torch.tensor([[1000, 300, 60, 60, 5]], dtype = torch.float32)\n",
        "output = model(test_input)\n",
        "_, predicted = torch.max(output, 1)\n",
        "print(f\"prediction: {predicted.item()}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
